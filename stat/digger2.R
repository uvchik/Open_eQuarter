#####################################################################################################################
#                                                                                                                   
# Project:      Open eQuarter 
#
# Part:         STAT: Digger Tools
#
# Status:       Active
#
# Author:       Werner Kaul
#
# Date:         21.10.2014
#
# Descrription: 
# The investigation of Open eQuarter Stat is drawing on the results of the demographic survey "Zensus 2011" 
# conducted by the Federal Statistical Office of Germany in 2011. This module includes functions to
# dig for correlations
#
######################################################################################################################
# basic includes
library(shiny)
source("config/plotpalettes.R")
source("init.R")
source("mun_db.R")
source("bld_db.R")


# Constructor for a single regression model 1
setConstructorS3("OeQ_Model", function(
  name="",
  verb_formula_epr="", # verbose formula (expression)
  verb_formula="",     # verbose formula (R-string)
  verb_formula_py="",  # verbose formula (py-string)
  ...){ # Constructor for a single regression model 1
  verbose("OeQ_Model: construct **********",1)
  this <- extend(Object(), "OeQ_Model",         # model name
                 .name=name,                   # model
                 .model=NULL,                   # model
                 .predict=NULL,                 # prediction function
                 .sqderiv1=0,                     # square derivation
                 .sqderiv=0,                     # square derivation
                 .rangewidth=0,
                 .validity=0,
                 .mode="lin",
                 .verb_formula_epr=expression(verb_formula_epr),         # verbose formula (expression)
                 .verb_formula=verb_formula,
                 .verb_formula_py=verb_formula_py
  )                   # verbose formula (string)
  this;
})
# Constructor for an regression object
setConstructorS3("OeQ_Regr", function(parent=NULL,columnname="",...){
  verbose("OeQ_Regr: construct **********",1)
  this <- extend(Object(), "OeQ_Regr",               
                 .parent=parent,
                 .columnname=columnname,# object name
                 .spline=OeQ_Model(),                # spline model
                 .splinelog=OeQ_Model(),             # log spline model
                 .smoothspline=OeQ_Model(),          # spline model
                 .smoothsplinelog=OeQ_Model(),       # log spline model
                 .lm1=OeQ_Model(),                   # quadratic linear model
                 .lm1log=OeQ_Model(),                # quadratic linear model
                 .lm1_splined=OeQ_Model(),           # quadratic linear model
                 .lm1log_splined=OeQ_Model(),                   # quadratic linear model
                 .lm1_logsplined=OeQ_Model(),           # quadratic linear model
                 .lm1log_logsplined=OeQ_Model(),                   # quadratic linear model
                 .lm2=OeQ_Model(),                   # cubic linear modell
                 .lm2log=OeQ_Model(),                # cubic linear modell
                 .lm2_splined=OeQ_Model(),           # cubic linear modell
                 .lm2log_splined=OeQ_Model(),        # cubic linear modell
                 .lm2_logsplined=OeQ_Model(),           # cubic linear modell
                 .lm2log_logsplined=OeQ_Model(),        # cubic linear modell
                 .bestfit="",
                 .estimate=NULL)
this;
})

#get bestfit as OeQ_Model
setMethodS3("bestfit", "OeQ_Regr", function(this,...){return(this[[this$.bestfit]])})
  
#get prediction of the best fit as vector or data.frame
setMethodS3("bestpredict", "OeQ_Regr", function(this,x,y,asdf=TRUE,...){return(this[[this$.bestfit]]$.predict(x,asdf))})

#get prediction of the best fit as vector or data.frame
setMethodS3("coeff", "OeQ_Regr", function(this,...){return(this[[this$.bestfit]]$.model$coefficients)})

#get coeff string of the best fit as vector or data.frame
setMethodS3("verbose_coeff", "OeQ_Regr", function(this,...){
  l.coeff=this[[this$.bestfit]]$.model$coefficients
  cat("Const= ",formatC(l.coeff[1],digits=12),
      "\na= ",formatC(l.coeff[2],digits=12),
      "\nb= ",formatC(l.coeff[3],digits=12),
      "\nc= ",formatC(l.coeff[4],digits=12),
      "\nd= ",formatC(l.coeff[5],digits=12))
})

#get prediction of the best fit as vector or data.frame
setMethodS3("verb_prn", "OeQ_Regr", function(this,...){return(this[[this$.bestfit]]$.verb_formula_epr)})

# generate R_Function snippet for the bestfit model of this regression
setMethodS3("generate_bestfit_correlation_code_snippet_in_R", "OeQ_Regr", function(this,par_name=NULL,...){
  if(is.null(par_name)) par_name = this$.columnname
  l.coeff=this[[this$.bestfit]]$.model$coefficients
  l.coeff[is.na(l.coeff)]=0
  return(this[[this$.bestfit]]$generate_correlation_code_snippet_in_R(par_name,VERBOSE[par_name,]$description))
})


# generate generate_bestfit_correlation_code_snippet_in_python snippet for the bestfit model of this regression
setMethodS3("generate_bestfit_correlation_code_snippet_in_python", "OeQ_Regr", function(this,par_name=NULL,...){
  if(is.null(par_name)) par_name = this$.columnname
  l.coeff=this[[this$.bestfit]]$.model$coefficients
  l.coeff[is.na(l.coeff)]=0
  return(this[[this$.bestfit]]$generate_correlation_code_snippet_in_python(par_name,VERBOSE[par_name,]$description))
})




# initialize regression models of the regression object
setMethodS3("init", "OeQ_Regr", function(this,limit=NULL,plt_title="",...){
  
  #getting proportionalized x,y data from the Investigation object
  x=this$.parent$.proportionalized["mean",this$.parent$.keycolumn,]
  y=this$.parent$.proportionalized["mean",this$.columnname,]
  
  # add an additonal data pair to the end if a limit is given
  if(!is.null(limit)) {
    x=c(x,limit[1])
    y=c(y,limit[2])
  }
  
  #set x values for regression to a regular sequence of 1000 elements
  l.x_out=seq(min(x),max(x),length.out=500)
  
  verbose("OeQ_Regr: init **********",1)
  
  #Reference model 1: Natural spline on f(x) ###################
  verbose("Natural spline...",0)
  this$.spline$.name=".spline"
  this$.spline$.verb_formula_epr="Natural Spline"       
  this$.spline$.verb_formula="Natural Spline"                  
  this$.spline$.verb_formula_py="Natural Spline"                  
  this$.spline$.model<-splinefun(x,y,method = "natural")
  this$.spline$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(this$.spline$.model(x))
    return(data.frame(x=x,y=this$.spline$.model(x),stringsAsFactors=FALSE))
  }                 
  
  #Square sum of deviation for best fit selection 
  this$.spline$.sqderiv=sum((y-this$.spline$.predict(x,asdf=FALSE))^2)                    
  
  #Reference model 2: Natural spline on f(log(x))  ###################
  verbose("Natural spline (log)...",1)
  this$.splinelog$.name=".splinelog"
  this$.splinelog$.verb_formula_epr="Natural Spline (Log)" 
  this$.splinelog$.verb_formula="Natural Spline (Log)"
  this$.splinelog$.verb_formula_py="Natural Spline (Log)"
  
  this$.splinelog$.model<-splinefun(log(x),y,method = "natural")
  this$.splinelog$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(this$.spline$.model(x))
    return(data.frame(x=x,y=this$.spline$.model(x),stringsAsFactors=FALSE))
  }         
  
  #Square sum of deviation for best fit selection 
  this$.splinelog$.sqderiv=sum((y-this$.splinelog$.predict(x,asdf=FALSE))^2)                     
  
  
  #Reference model 3: Smooth spline on f(x) ###################
  verbose("Smooth spline...",1)
  this$.smoothspline$.name=".smoothspline"
  this$.smoothspline$.verb_formula_epr="Smooth Spline"   
  this$.smoothspline$.verb_formula="Smooth Spline" 
  this$.smoothspline$.verb_formula_py="Smooth Spline"
  
  print(data.frame(x,y))
  y=y[is.finite(log(x))]
  x=x[is.finite(log(x))]
  x=x[is.finite(y)]
  y=y[is.finite(y)]
  x[x==0]=1e-6
  
  print(cbind(x,y))
  this$.smoothspline$.model<-smooth.spline(x,y,spar=1,tol=1e-10)
  this$.smoothspline$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.smoothspline$.model,x)$y)
    return(data.frame(x=x,y=predict(this$.smoothspline$.model,x)$y,stringsAsFactors=FALSE))
  }                 
  
  #Square sum of deviation for best fit selection 
  this$.smoothspline$.sqderiv=sum((y-this$.smoothspline$.predict(x,asdf=FALSE))^2)                     
  this$.smoothspline$.validity=(max(this$.smoothspline$.predict(l.x_out,asdf=FALSE))<max(y))+(min(this$.smoothspline$.predict(l.x_out,asdf=FALSE))>min(y))
  
  
  #Reference model 4: Smooth spline on f(log(x)) ###################
  verbose("Smooth spline (log)...",1)
  this$.smoothsplinelog$.name=".smoothsplinelog"
  this$.smoothsplinelog$.verb_formula_epr="Smooth Spline (Log)"             
  this$.smoothsplinelog$.verb_formula="Smooth Spline (Log)"      
  this$.smoothsplinelog$.verb_formula_py="Smooth Spline (Log)"  
  this$.smoothsplinelog$.verb_formula_py="Smooth Spline (Log)"
  
  this$.smoothsplinelog$.model<-smooth.spline(log(x),y,spar=0.7,tol=1e-6)
  this$.smoothsplinelog$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.smoothsplinelog$.model,log(x))$y)
    return(data.frame(x=x,y=predict(this$.smoothsplinelog$.model,log(x))$y,stringsAsFactors=FALSE))
  }                 
  
  #Square sum of deviation for best fit selection 
  this$.smoothsplinelog$.sqderiv=sum((y-this$.smoothsplinelog$.predict(x,asdf=FALSE))^2)                     
  this$.smoothsplinelog$.validity=(max(this$.smoothsplinelog$.predict(l.x_out,asdf=FALSE))<max(y))+(min(this$.smoothsplinelog$.predict(l.x_out,asdf=FALSE))>min(y))
  
  
  
  #alternatively use smoothline of f(log(x)) which emerged to deliver more reliable bestfit decisions
  l.y_comp=this$.smoothsplinelog$.predict(l.x_out,asdf=FALSE)
  
  
  #Regression model 1: Linear model polynomal regression (deg=3) on f(x) ###################
  verbose("Linear model polynomal regression (deg=3)...",1)
  this$.lm1$.name=".lm1"
  this$.lm1$.mode="lin"
  this$.lm1$.verb_formula_epr=expression(paste("y = Const ",+ ax + bx^2 + cx^3))        
  this$.lm1$.verb_formula="y = Const + a*x + b*x^2 + c*x^3"      
  this$.lm1$.verb_formula_py="    y = Const + a*x + b*x**2 + c*x**3" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm1$.name)
 # x=l.x_out
 # y=l.y_comp
  this$.lm1$.model<-lm(y ~ poly(x,3,raw=TRUE))
  this$.lm1$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm1$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm1$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
  l.prediction=this$.lm1$.predict(l.x_out,asdf=FALSE)
  
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
  this$.lm1$.sqderiv=sum((l.y_comp-l.prediction)^2)      
  
   #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm1$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Initialize the regressions bestfit entry
  this$.bestfit=this$.lm1$.name
  
  
  #Regression model 2: Linear model polynomal regression (deg=3) on f(log(x) ###################
  verbose("Linear model polynomal regression (deg=3,log)...",1)
  this$.lm1log$.name=".lm1log"
  this$.lm1log$.mode="log"
  this$.lm1log$.verb_formula_epr=expression(paste("y = Const + a",tilde(x)," + b ",tilde(x)^2," + c",tilde(x)^3," ; ",tilde(x)," = ln(x) "))        
  this$.lm1log$.verb_formula="x=log(xin)\ny = Const + a*x + b*x^2 + c*x^3"      
  this$.lm1log$.verb_formula_py="    x=math.log(xin)\n    y = Const + a*x + b*x**2 + c*x**3" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm1log$.name)
  
# this$.lm1log$.model<-lm(y ~ poly(log(x),3,raw=TRUE))
 this$.lm1log$.model<-lm(y ~ I(log(x))+I(log(x)^2)+I(log(x)^3))
 this$.lm1log$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm1log$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm1log$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
  l.prediction=this$.lm1log$.predict(l.x_out,asdf=FALSE)
  
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm1log$.sqderiv=sum((l.y_comp-l.prediction)^2)                     
      
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm1log$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
 # if((this$.lm1log$.sqderiv < this[[this$.bestfit]]$.sqderiv)&&(this$.lm1log$.validity >= this[[this$.bestfit]]$.validity))  { 
  if(!is.na(this$.lm1log$.sqderiv)){
    if((this$.lm1log$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm1log$.validity >= this[[this$.bestfit]]$.validity))  { 
      
      this$.bestfit=this$.lm1log$.name
    }
  }
  
  #Regression model 3: Linear model polynomal regression (deg=4) on f(x) ###################
  verbose("Linear model polynomal regression (deg=4)...",1)
  this$.lm2$.name=".lm2"
  this$.lm2$.mode="lin"
  this$.lm2$.verb_formula_epr=expression(paste("y = Const ",+ ax + bx^2 + cx^3 + dx^4))        
  this$.lm2$.verb_formula="y = Const + a*x + b*x^2 + c*x^3 + d*x^4"      
  this$.lm2$.verb_formula_py="    y = Const + a*x + b*x**2 + c*x**3 + d*x**4" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm2$.name)
  
  this$.lm2$.model<-lm(y ~ poly(x,4,raw=TRUE))
  this$.lm2$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm2$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm2$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
 l.prediction=this$.lm2$.predict(l.x_out,asdf=FALSE)
 
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm2$.sqderiv=sum((l.y_comp-l.prediction)^2)                     
     
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm2$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if((this$.lm2$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm2$.validity >= this[[this$.bestfit]]$.validity))  { 
    this$.bestfit=this$.lm2$.name
  }
  
  
  #Regression model 4: Linear model polynomal regression (deg=4) on f(log(x)) ###################
  verbose("Linear model polynomal regression (deg=4,log)...",1)
  this$.lm2log$.name=".lm2log"
  this$.lm2log$.mode="log"
  this$.lm2log$.verb_formula_epr=expression(paste("y = Const + a",tilde(x)," + b ",tilde(x)^2," + c",tilde(x)^3," + d",tilde(x)^4," ; ",tilde(x)," = ln(x) "))        
  this$.lm2log$.verb_formula="x=log(xin)\ny = Const + a*x + b*x^2 + c*x^3  + d*x^4"      
  this$.lm2log$.verb_formula_py="    x=math.log(xin)\n    y = Const + a*x + b*x**2 + c*x**3 + d*x**4" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm2log$.name)
  
  this$.lm2log$.model<-lm(y ~ poly(log(x),4,raw=TRUE))
  this$.lm2log$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm2log$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm2log$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
 l.prediction=this$.lm2log$.predict(l.x_out,asdf=FALSE)
 
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
this$.lm2log$.sqderiv=sum((l.y_comp-l.prediction)^2)                     
 
  
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm2log$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if(!is.na(this$.lm2log$.sqderiv)){
    if((this$.lm2log$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm2log$.validity >= this[[this$.bestfit]]$.validity ))  { 
    this$.bestfit=this$.lm2log$.name
    }
  }
  
  
  #backing up the samples
  x=l.x_out
  y=l.y_comp


  #for the following model use the smoothspline results for regression
  x=l.x_out
  y=this$.smoothspline$.predict(x,asdf=FALSE)
  
  
  #Regression model 5: Linear model polynomal regression (deg=3) on the smoothspline of f(x) ###################
  verbose("Linear model polynomal regression (deg=3 splined)...",1)
  this$.lm1_splined$.name=".lm1_splined"
  this$.lm1_splined$.mode="lin"
  this$.lm1_splined$.verb_formula_epr=expression(paste("y = Const ",+ ax + bx^2 + cx^3))        
  this$.lm1_splined$.verb_formula="y = Const + a*x + b*x^2 + c*x^3"      
  this$.lm1_splined$.verb_formula_py="    y = Const + a*x + b*x**2 + c*x**3 " 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm1_splined$.name)
  
  this$.lm1_splined$.model<-lm(y ~ poly(x,3,raw=TRUE))
  this$.lm1_splined$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm1_splined$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm1_splined$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
 l.prediction=this$.lm1_splined$.predict(l.x_out,asdf=FALSE)
 
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm1_splined$.sqderiv=sum((l.y_comp-l.prediction)^2)                     
  
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm1_splined$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if((this$.lm1_splined$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm1_splined$.validity >= this[[this$.bestfit]]$.validity ))  { 
    this$.bestfit=this$.lm1_splined$.name
  }
  
  
  #Regression model 6: Linear model polynomal regression (deg=3) on the smoothspline of f(log(x)) ###################
  verbose("Linear model polynomal regression (deg=3,log splined)...",1)
  this$.lm1log_splined$.name=".lm1log_splined"
  this$.lm1log_splined$.mode="log"
  this$.lm1log_splined$.verb_formula_epr=expression(paste("y = Const + a",tilde(x)," + b ",tilde(x)^2," + c",tilde(x)^3," ; ",tilde(x)," = ln(x) "))        
  this$.lm1log_splined$.verb_formula="x=log(xin)\ny = Const + a*x + b*x^2 + c*x^3"      
  this$.lm1log_splined$.verb_formula_py="    x=math.log(xin)\n    y = Const + a*x + b*x**2 + c*x**3" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm1log_splined$.name)
  y=y[!is.nan(log(x))]
  x=x[!is.nan(log(x))]
  x=x[!is.nan(y)]
  y=y[!is.nan(y)]
  x[x==0]=1e-6
 #print(cbind(x,y))
  #stop()
  
  this$.lm1log_splined$.model<-lm(y ~ poly(log(x),3,raw=TRUE))
  this$.lm1log_splined$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm1log_splined$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm1log_splined$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
  l.prediction=this$.lm1log_splined$.predict(x,asdf=FALSE)
 
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm1log_splined$.sqderiv=sum((l.y_comp-l.prediction)^2)                     
 
  
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm1log_splined$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if((this$.lm1log_splined$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm1log_splined$.validity >= this[[this$.bestfit]]$.validity))  { 
    this$.bestfit=this$.lm1log_splined$.name
  }
  
  
  #Regression model 7: Linear model polynomal regression (deg=4) on the smoothspline of f(x) ###################
  verbose("Linear model polynomal regression (deg=4 splined)...",1)
  this$.lm2_splined$.name=".lm2_splined"
  this$.lm2_splined$.mode="lin"
  this$.lm2_splined$.verb_formula_epr=expression(paste("y = Const ",+ ax + bx^2 + cx^3 + dx^4))        
  this$.lm2_splined$.verb_formula="y = Const + a*x + b*x^2 + c*x^3 + d*x^4"      
  this$.lm2_splined$.verb_formula_py="    y = Const + a*x + b*x**2 + c*x**3 + d*x**4" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm2_splined$.name)
  
  this$.lm2_splined$.model<-lm(y ~ poly(x,4,raw=TRUE))
  this$.lm2_splined$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm2_splined$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm2_splined$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
  l.prediction=this$.lm2_splined$.predict(x,asdf=FALSE)
 
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm2_splined$.sqderiv=sum((l.y_comp-l.prediction)^2)                     
 
  #Setting the rangewidth of the prediction
  this$.lm2_splined$.rangewidth=max(l.prediction)-min(l.prediction)
  
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm2_splined$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if((this$.lm2_splined$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm2_splined$.validity >= this[[this$.bestfit]]$.validity ))  { 
    this$.bestfit=this$.lm2_splined$.name
  }
  
  
  #Regression model 8: Linear model polynomal regression (deg=4) on the smoothspline of f(log(x) ###################
  verbose("Linear model polynomal regression (deg=4,log splined)...",1)
  this$.lm2log_splined$.name=".lm2log_splined"
  this$.lm2log_splined$.mode="log"
  this$.lm2log_splined$.verb_formula_epr=expression(paste("y = Const + a",tilde(x)," + b ",tilde(x)^2," + c",tilde(x)^3," + d",tilde(x)^4," ; ",tilde(x)," = ln(x) "))        
  this$.lm2log_splined$.verb_formula="x=log(xin)\ny = Const + a*x + b*x^2 + c*x^3  + d*x^4"      
  this$.lm2log_splined$.verb_formula_py="    x=math.log(xin)\n    y = Const + a*x + b*x**2 + c*x**3 + d*x**4" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm2log_splined$.name)
  
  this$.lm2log_splined$.model<-lm(y ~ poly(log(x),4,raw=TRUE))
  this$.lm2log_splined$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm2log_splined$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm2log_splined$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
  l.prediction=this$.lm2log_splined$.predict(x,asdf=FALSE)
 
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm2log_splined$.sqderiv=sum((l.y_comp-l.prediction)^2)       
 
 #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm2log_splined$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if((this$.lm2log_splined$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm2log_splined$.validity >= this[[this$.bestfit]]$.validity ))  { 
    this$.bestfit=this$.lm2log_splined$.name
  }
  
  #Regression model 9: Linear model polynomal regression (deg=3) on the logarithm of smoothspline of f(x) ###################
  verbose("Linear model polynomal regression (deg=3 logsplined)...",1)
  this$.lm1_logsplined$.name=".lm1_logsplined"
  this$.lm1_logsplined$.mode="lin"
  this$.lm1_logsplined$.verb_formula_epr=expression(paste("y = Const ",+ ax + bx^2 + cx^3))        
  this$.lm1_logsplined$.verb_formula="y = Const + a*x + b*x^2 + c*x^3"      
  this$.lm1_logsplined$.verb_formula_py="    y = Const + a*x + b*x**2 + c*x**3" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm1_logsplined$.name)
  
  this$.lm1_logsplined$.model<-lm(y ~ poly(x,3,raw=TRUE))
  this$.lm1_logsplined$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm1_logsplined$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm1_logsplined$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
  l.prediction=this$.lm1_logsplined$.predict(x,asdf=FALSE)
 
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm1_logsplined$.sqderiv=sum((l.y_comp-l.prediction)^2)                     
 
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm1_logsplined$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if((this$.lm1_logsplined$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm1_logsplined$.validity >= this[[this$.bestfit]]$.validity ))  { 
    this$.bestfit=this$.lm1_logsplined$.name
  }
  
  
  #Regression model 10: Linear model polynomal regression (deg=3) on the logarithm of smoothspline of f(log(x) ###################
  verbose("Linear model polynomal regression (deg=3,log logsplined)...",1)
  this$.lm1log_logsplined$.name=".lm1log_logsplined"
  this$.lm1log_logsplined$.mode="log"
  this$.lm1log_logsplined$.verb_formula_epr=expression(paste("y = Const + a",tilde(x)," + b ",tilde(x)^2," + c",tilde(x)^3," ; ",tilde(x)," = ln(x) "))        
  this$.lm1log_logsplined$.verb_formula="x=log(xin)\ny = Const + a*x + b*x^2 + c*x^3 "      
  this$.lm1log_logsplined$.verb_formula_py="    x=math.log(xin)\n    y = Const + a*x + b*x**2 + c*x**3 " 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm1log_logsplined$.name)
  
  this$.lm1log_logsplined$.model<-lm(y ~ poly(log(x),3,raw=TRUE))
  this$.lm1log_logsplined$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm1log_logsplined$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm1log_logsplined$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
  l.prediction=this$.lm1log_logsplined$.predict(x,asdf=FALSE)
 
  
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm1log_logsplined$.sqderiv=sum((l.y_comp-l.prediction)^2)                     
 
  
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm1log_logsplined$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if((this$.lm1log_logsplined$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm1log_logsplined$.validity >= this[[this$.bestfit]]$.validity))  { 
    this$.bestfit=this$.lm1log_logsplined$.name
  }
  
  
  #Regression model 11: Linear model polynomal regression (deg=4) on the logarithm of smoothspline of f(x) ###################
  verbose("Linear model polynomal regression (deg=4 logsplined)...",1)
  this$.lm2_logsplined$.name=".lm2_logsplined"
  this$.lm2_logsplined$.mode="lin"
  this$.lm2_logsplined$.verb_formula_epr=expression(paste("y = Const ",+ ax + bx^2 + cx^3 + dx^4))        
  this$.lm2_logsplined$.verb_formula="y = Const + a*x + b*x^2 + c*x^3 + d*x^4"      
  this$.lm2_logsplined$.verb_formula_py="    y = Const + a*x + b*x**2 + c*x**3 +d*x**4" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm2_logsplined$.name)
  
  this$.lm2_logsplined$.model<-lm(y ~ poly(x,4,raw=TRUE))
  this$.lm2_logsplined$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm2_logsplined$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm2_logsplined$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
  l.prediction=this$.lm2_logsplined$.predict(x,asdf=FALSE)
 
  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm2_logsplined$.sqderiv=sum((l.y_comp-l.prediction)^2)      
 
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm2_logsplined$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if((this$.lm2_logsplined$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm2_logsplined$.validity >= this[[this$.bestfit]]$.validity))  { 
    this$.bestfit=this$.lm2_logsplined$.name
  }
  
  
  #Regression model 12: Linear model polynomal regression (deg=4) on the logarithm of smoothspline of f(log(x)) ###################
  verbose("Linear model polynomal regression (deg=4,log logsplined)...",1)
  this$.lm2log_logsplined$.name=".lm2log_logsplined"
  this$.lm2log_logsplined$.mode="log"
  this$.lm2log_logsplined$.verb_formula_epr=expression(paste("f ( ",tilde(x)," ) = Const + a",tilde(x)," + b ",tilde(x)^2," + c",tilde(x)^3," + d",tilde(x)^4," ; ",tilde(x)," = ln(x) "))        
  this$.lm2log_logsplined$.verb_formula="x=log(xin)\ny = Const + a*x + b*x^2 + c*x^3  + d*x^4"      
  this$.lm2log_logsplined$.verb_formula_py="    x=math.log(xin)\n    y = Const + a*x + b*x**2 + c*x**3 +d*x**4" 
  
  #Add this regression to the list
  this$.allregr=c(this$.allregr,this$.lm2log_logsplined$.name)
  
  this$.lm2log_logsplined$.model<-lm(y ~ poly(log(x),4,raw=TRUE))
  this$.lm2log_logsplined$.predict= function(x,asdf=TRUE) {
    if(asdf==FALSE) return(predict(this$.lm2log_logsplined$.model,newdata=data.frame(x=x)))
    return(data.frame(x=x,y=predict(this$.lm2log_logsplined$.model,newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  }
  
  #Setting prediction data for the best fit survey 
  l.prediction=this$.lm2log_logsplined$.predict(x,asdf=FALSE)

  #Square sum of deviation for best fit selection (to smoothline of f(log(x)) which emerged to deliver the most reliable comparative values )
 this$.lm2log_logsplined$.sqderiv=sum((l.y_comp-l.prediction)^2)                       
 
  #Validity of the regression is done by a range check: 
  #0: regression is exceeding the sample spline max and undercutting the sample spline min
  #1: regression is undercutting the sample spline min
  #2: regression is exceeding the sample spline max
  #3: regression is not exceeding or ondercutting the range of the sample spline
  this$.lm2log_logsplined$.validity=2*(max(l.prediction)<max(y))+(min(l.prediction)>min(y))
  
  #Set best fit to this model if square deviation sum is smaller and validity is the same or better
  if((this$.lm2log_logsplined$.sqderiv < this[[this$.bestfit]]$.sqderiv)){#&&(this$.lm2log_logsplined$.validity >= this[[this$.bestfit]]$.validity ))  { 
    this$.bestfit=this$.lm2log_logsplined$.name
  }
x=this$.parent$.proportionalized["mean",this$.parent$.keycolumn,]
y=this$.parent$.proportionalized["mean",this$.columnname,]

hilfsplot=FALSE
if(hilfsplot){
  plot(x,y,xlim=c(min(l.x_out),max(l.x_out)),main=paste("Regression results ’",this$.columnname,"'",sep=""))#ylim=c(0,1),
  lines(this$.lm1$.predict(l.x_out),col="BLUE",lty=1)
  lines(this$.lm1log$.predict(l.x_out),col="BLUE",lty=2)
  lines(this$.lm2$.predict(l.x_out),col="BLUE",lty=5)
  lines(this$.lm2log$.predict(l.x_out),col="BLUE",lty=4)
  lines(this$.lm1_splined$.predict(l.x_out),col="RED",lty=1)
  lines(this$.lm1log_splined$.predict(l.x_out),col="RED",lty=2)
  lines(this$.lm2_splined$.predict(l.x_out),col="RED",lty=5)
  lines(this$.lm2log_splined$.predict(l.x_out),col="RED",lty=4)
  lines(this$.lm1_logsplined$.predict(l.x_out),col="GREEN",lty=1)
  lines(this$.lm1log_logsplined$.predict(l.x_out),col="GREEN",lty=2)
  lines(this$.lm2_logsplined$.predict(l.x_out),col="GREEN",lty=5)
  lines(this$.lm2log_logsplined$.predict(l.x_out),col="GREEN",lty=4)
  lines(this[[this$.bestfit]]$.predict(l.x_out),col="BLACK",lwd=3)
  lines(this$.spline$.predict(l.x_out),col="CYAN",lwd=2)
  #lines(this$.splinelog$.predict(l.x_out),col="ORANGE",lwd=2,lty=2)
  # lines(this$.smoothspline$.predict(l.x_out),col="DARK GREY",lwd=2)
  lines(this$.smoothsplinelog$.predict(l.x_out),col="ORANGE",lwd=2)
  l.leg_names=character(0)
  for (i in this$.allregr) l.leg_names=c(l.leg_names,paste(i,this[[i]]$.validity,
                                                           round(this[[i]]$.sqderiv,2),
                                                           round(this[[i]]$.sqderiv1*100,2),
                                                           round(this[[i]]$.sqderiv/this[[i]]$.sqderiv1,2)))
  legend('topright', c(l.leg_names,paste("bestfit -> ",this$.bestfit,sep="")) , 
         lwd=c(rep(1,12),3),
         lty=c(1,2,5,4,1,2,5,4,1,2,5,4,1), col=c('black','blue','blue','blue','blue','red', 'red', 'red',' red','green','green','green','green','black','cyan','orange'), bty='n', cex=.75)
}
this
###########################################
})




# Constructor for an Investigation Object, holding all relevant data
setConstructorS3("OeQ_Inv", function(...){
  verbose("OeQ_Inv: construct **********",1)
  this <- extend(Object(), "OeQ_Inv",         # object name
                 .type="log",                 # assumed type of correlation
                 .data=data.frame(0),         # source data
                 .proportionalized=list(0),   # proportionial resampled data including quantiles
                 .keycolumn=NULL,             # key column for which a correlation should reference to
                 .normcolumn=NULL,            # name of the norm column, typically the sum column
                 .xrange=NULL,                # range filter for the key column
                 .p_mode="auto",              # proportionalizing mode, "auto", "log" or "lin"
                 .palette=PAL_PRINT,          # palette for graphic output, "PAL_PRINT" or "PAL_SCREEN"
                 .n_breaks=NULL,              # number of breaks for resampling
                 .p_breaks=NULL,              # break distances
                 .regressions=list())
this;
})

# Initializing an Investigation object
setMethodS3("init", "OeQ_Inv", function(this,
                                        source_db,
                                        keycolumn=NULL,
                                        normcolumn=NULL,
                                        n_breaks=NULL,
                                        xrange=NULL,
                                        limits=NULL,
                                        p_mode="auto",
                                        type="log",
                                        palette=PAL_PRINT,
                                        ...) {
  verbose("OeQ_Inv: init **********",1)
  #if no keycolumn defined use first column as key
  if(is.null(keycolumn)) keycolumn=colnames(source_db)[1]
  this$.keycolumn=keycolumn
  
  #columns to ignore
  l.exlude=c(keycolumn,"n","SUM")  
  #drop rows if value in keycolumn is NA
  source_db=source_db[!is.na(source_db[,keycolumn]),] 
  
  if(!is.null(normcolumn)) {
    #ad normcolumn to ignore list
    l.exlude=c(l.exlude,normcolumn) 
    # drop rows if value in normcolumn is NA
    source_db=source_db[!is.na(source_db[,normcolumn]),]
    # divide all column except the keycolumn by the norm column
    source_db[,colnames(source_db)!=keycolumn]=source_db[,colnames(source_db)!=keycolumn]/source_db[,normcolumn]
  }
  this$.normcolumn=normcolumn
  #sort samplebase by keycolumn
  source_db=source_db[order(source_db[,keycolumn]),] 
  #if xrange is not defined use min and max of keycolumn
  if(is.null(xrange)) xrange=c(min(source_db[,keycolumn]), max(source_db[,keycolumn]))
  if((xrange[1]<min(source_db[,keycolumn]))|(xrange[1]==-1)) xrange[1]=min(source_db[,keycolumn])
  if((xrange[2]>max(source_db[,keycolumn]))|(xrange[2]==-1)) xrange[2]=max(source_db[,keycolumn])
  this$.xrange=xrange
  #drop rows out of range
  source_db=source_db[(source_db[,keycolumn]>=xrange[1])&(source_db[,keycolumn]<=xrange[2]),] 
  this$.data=source_db
  #if sum is correct (normcol)set NA to 0
  if(!is.null(normcolumn)) {l.sumcheck=rowSums(this$.data[colnames(this$.data)[!(colnames(this$.data) %in% l.exlude)]],na.rm = TRUE)==this$.data[,this$.normcolumn]
                            for(r in which(l.sumcheck)) this$.data[r,is.na(this$.data[r,])]=0}
  
  #if number of breaks is not defined then calculate
  if(is.null(n_breaks)) n_breaks=5*log(nrow(this$.data))
  this$.n_breaks=n_breaks
  
#  #calculate breaks default mode
  print(this      )
#  exit
  this$.p_breaks=as.vector(table(cut(this$.data[,keycolumn],n_breaks,include.lowest=T)))
  this$.p_mode="lin"
  #calculate breaks auto mode
  if(p_mode=="auto")  {
    #if max is found in the first 25% of the orderer values the use log 
    print(this$.data[1:10,keycolumn])
    if(which(this$.p_breaks==max(this$.p_breaks))[1]<0.25*length(this$.p_breaks))  {
      # avoid -Inf values by setting 0 to 1e-99
      l.tmpdata=this$.data[,keycolumn]
      l.tmpdata[l.tmpdata==0]<-1e-99
      print(log(l.tmpdata[1:10]))
      #exit
      this$.p_breaks=as.vector(table(cut(log(l.tmpdata),n_breaks,include.lowest=T)))
      this$.p_mode="log"
    }
  }
  #calculate breaks log mode
  if(p_mode=="log")  this$.p_breaks=as.vector(table(cut(log(this$.data[,keycolumn]),n_breaks,include.lowest=T)))
  
  # set output palette
  this$.palette=palette
  
  # set assumed corellation type
  this$.type=type
  
  this$.proportionalized =this$proportionalize()
  # new dataset without key and norm column
  l.incl_names=colnames(this$.data)[!(colnames(this$.data) %in% l.exlude)]
  for (i in l.incl_names){
    x=this$.proportionalized["mean",this$.keycolumn,]
    print(i)
    y=this$.proportionalized["mean",i,]
    print(length(x))
    print(length(y))
    this$.regressions[[i]]=OeQ_Regr(this,i)$init(limit=c(limits$KEY,limits[[i]]))
  }
  # initializing the models as lists
  this
})



#Aufruf:
#1. OeQ_Inv() neues Objekt
#2. OeQ_Inv$init() Objekt initialisieren
#3. OeQ_Inv$proportionalize() proportionalisierung durchführen
#4. OeQ_Inv$regression() Regressionen durchführen


new_OeQ_Inv<-function(                 source_db,
                                       keycolumn=NULL,
                                       normcolumn=NULL,
                                       n_breaks=NULL,
                                       xrange=NULL,
                                       p_mode="auto",
                                       palette=PAL_PRINT,
                                       limits=NULL,
                                       type="log",
                                       pdffile=NULL,
                                       ...) 
{
  verbose("new_OeQ_Inv **********",1)
  l.obj=OeQ_Inv()
   l.obj$init(source_db=source_db,
             keycolumn=keycolumn,
             normcolumn=normcolumn,
             n_breaks=n_breaks,
             xrange=xrange,
             p_mode=p_mode,
             limits=limits,
             palette=palette,
             type=type,
             ...)
  #l.obj$proportionalize()
  #l.obj=l.obj$regression_combined(columns=NULL)
#  l.obj$distribution_plot(palette=PAL_PRINT,pdffile=pdffile)
#  l.obj$sum_plot(palette=PAL_PRINT)
  l.obj
}





setMethodS3("print", "OeQ_Inv", function(this,...){
  cat("\n$.data -->\n")
  print(this$.data[1:10,])
  cat("\n<-- end of $.data, ",nrow(this$.data)-10," rows of ",nrow(this$.data)," omitted!\n\n")
  cat("\n$.proportionalized -->\n")
  print(this$.proportionalized)
  cat("<-- end of $.proportionalized\n")
  cat("\n$.keycolumn\n")
  print(this$.keycolumn)
  cat("\n$.normcolumn\n")
  print(this$.normcolumn)
  cat("\n$.xrange\n")
  print(this$.xrange)
  cat("\n$.p_mode\n")
  print(this$.p_mode)
  cat("\n$.palette -->\n")
  print(this$.palette)
  cat("<-- end of $.palette\n")
  cat("\n$.n_breaks\n")
  print(this$.n_breaks)
  cat("\n$.p_breaks\n")
  print(this$.p_breaks)
  cat("\n$.lm1 -->\n")
  print(this$.lm1)
  cat("\n<-- end of $.lm1\n\n")
  cat("\n$.lm2 -->\n")
  print(this$.lm2)
  cat("\n<-- end of $.lm2\n\n")
  cat("\n$.nls1 -->\n")
  #  for (i in names(this$.nls1) ){
  #    cat("$",i,"\n")
  #    print(this$.nls1[[i]]$getPars())
  #    cat("\n")
  #  }
  print(this$.nls1)
  cat("\n<-- end of $.nls1\n\n")
  cat("\n$.nls2 -->\n")
  # for (i in names(this$.nls2) ){
  #   cat("$",i,"\n")
  #   print(this$.nls2[[i]]$getPars())
  #   cat("\n")
  # }
  print(this$.nls2)
  cat("\n<-- end of $.nls2\n\n")
})

#split data frame at rows given in breaks
df_break<-function(df,breaks,resid=TRUE,addfirst=10,addlast=10){
  l.to=cumsum(breaks)
  if(l.to[length(l.to)]<nrow(df)) l.to=c(l.to,nrow(df))
  l.to=l.to[l.to<=nrow(df)]
  l.from=c(1,l.to[-length(l.to)]+1)
  l.ranges=cbind(l.from,l.to)
  if(!resid) l.ranges=l.ranges[-nrow(l.ranges),]
  l.broken=apply(l.ranges,1,function(y) return(df[y[1]:y[2],]))
  # for (i in 1:addlast) l.broken[[length(l.broken)+1]]=rbind(df[nrow(df),],df[nrow(df),])
  print( l.broken)
  l.broken
}


setMethodS3("proportionalize", "OeQ_Inv", function(this,...){
  verbose("OeQ_Inv: proportionalize **********",1)
  l.essential<-function(df){
    rbind(mean=colMeans(df),apply(df,2,function(y) quantile(y,na.rm=TRUE)))
  }
  l.broken=df_break(this$.data,this$.p_breaks) 
  l.ess=lapply(l.broken,l.essential)
  l.ess=array(unlist(l.ess),dim=c(dim(l.ess[[1]]),length(l.ess)),dimnames=list(rownames(l.ess[[1]]),colnames(l.ess[[1]])))
  this$.proportionalized=l.ess
  return(l.ess)
})

#AKTUELLE ABFRAGEN
#Number of flats
#KAL=new_OeQ_Inv(gebaeude_zensus_2011[,c("P_DENS",BUILDINGS_BY_NODWELL)],normcolumn="BwD_NODWELL_TOTAL",xrange=c(0,-1))
#KAL$distribution_plot(palette=PAL_PRINT,pdffile="pdfout/noflats_distribution.pdf",xrange=c(1,18000))
#KAL$sum_plot(palette=PAL_PRINT,pdffile="pdfout/noflats_distribution.pdf",xrange=c(1,18000))

#Number of Rooms
#KAL=new_OeQ_Inv(gebaeude_zensus_2011[,c("P_DENS",DWELLINGS_BY_NOROOMS)],normcolumn="D_NOROOMS_TOTAL",p_mode="lin",type="log",xrange=c(20,-1))
#KAL$distribution_plot(palette=PAL_PRINT,pdffile="pdfout/norooms_distribution.pdf",xrange=c(1,10000))
#KAL$sum_plot(palette=PAL_PRINT,pdffile="pdfout/norooms_sum.pdf",xrange=c(1,10000))

#Size of flats
#KAL=new_OeQ_Inv(gebaeude_zensus_2011[,c("P_DENS",DWELLINGS_BY_AREA)],normcolumn="D_AREA_TOTAL",p_mode="lin",type="log",xrange=c(20,-1))
#KAL$distribution_plot(palette=PAL_PRINT,pdffile="pdfout/flatsize_distribution.pdf",xrange=c(1,10000))
#KAL$sum_plot(palette=PAL_PRINT,pdffile="pdfout/flatsize_sum.pdf",xrange=c(1,10000))



setMethodS3("regression", "OeQ_Inv", function(this,
                                              columns=NULL,
                                              ...) {
  verbose("OeQ_Inv: regression **********",1)
  if(is.null(columns)) columns=names(this$.lm1)
  x=this$.proportionalized["mean",this$.keycolumn,]
  
  if(this$.type=="log") {
    # avoid infinite values on log by setting 0 to 1e-99
    x[x==0]<-1e-99
    x=log(x) 
  }
  #  plot(x)
  for (i in columns){
    y=this$.proportionalized["mean",i,]
    #  plot(exp(x),y)
    # if(!is.null(this$.normcolumn)) y=y/this$.data[,this$.normcolumn]  
    #nonlinear least square 1: constant and log(x)
    #   suppressWarnings({
    this$.nls1[[i]]<-nls(y ~ Const + A * x,start=list(Const=1,A=1),control=list(warnOnly=TRUE))
    #nonlinear least square 2: constant and x
    this$.nls2[[i]]<-nls(y ~ Const + A * x + B*I(x^2),start=list(Const=1,A=1,B=1),control=list(warnOnly=TRUE))
    #linear model 1: polnomial 2nd degree
    this$.lm1[[i]]<-lm(y ~ poly(x,2,raw=TRUE))
    #linear model 2: polnomial 3rd degree
    print(lm(y ~ poly(x,3,raw=TRUE)))
    this$.lm2[[i]]<-lm(y ~ poly(x,3,raw=TRUE))
    #  })
  }
  this
})

setMethodS3("leastsquaretest", "OeQ_Inv", function(this,column=NULL,...){
  verbose("OeQ_Inv: leastsquaretest **********",1)
  x=this$.proportionalized["mean",this$.keycolumn,]
  if (is.null(column)) {
    column=colnames(this$.proportionalized)
    column=column[column!=this$.keycolumn]
    column=column[column!=this$.normcolumn]
  }
  if(length(column==1)) column=c(column)
 # print(column)
  for (i in column) {
    y=this$.proportionalized["mean",column,]
    square=list()
 # square$spline=sum((y-this$predict_spline(x,column,asdf=FALSE))^2)
 # square$splinelog=sum((y-this$predict_splinelog(x1,column,asdf=FALSE))^2)
 print(this$predict_lm1(x,i,asdf=FALSE))
 print(y)
 square$.lm1=sum((y-this$predict_lm1(x,i,asdf=FALSE))^2)
  square$.lm1log=sum((y-this$predict_lm1log(x,i,asdf=FALSE))^2)
  square$.lm1_splined=sum((y-this$predict_lm1_splined(x,i,asdf=FALSE))^2)
  square$.lm1log_splined=sum((y-this$predict_lm1log_splined(x,i,asdf=FALSE))^2)
  square$.lm2=sum((y-this$predict_lm2(x,i,asdf=FALSE))^2)
  square$.lm2log=sum((y-this$predict_lm2log(x,i,asdf=FALSE))^2)
  square$.lm2_splined=sum((y-this$predict_lm2_splined(x,i,asdf=FALSE))^2)
  square$.lm2log_splined=sum((y-this$predict_lm2log_splined(x,i,asdf=FALSE))^2)
 square$.lm3=sum((y-this$predict_lm3(x,i,asdf=FALSE))^2)
 square$.lm3log=sum((y-this$predict_lm3log(x,i,asdf=FALSE))^2)
 square$.lm3_splined=sum((y-this$predict_lm3_splined(x,i,asdf=FALSE))^2)
 square$.lm3log_splined=sum((y-this$predict_lm3log_splined(x,i,asdf=FALSE))^2)
 bf=names(square)[square==min(unlist(square))][1]
 this$.bestfit[[i]]=this[[bf]][[i]]
  cat("Least SQUARE ",i," has: ",bf,"\n")
}
this$.bestfit
})
 

#function_calls
setMethodS3("predict_spline", "OeQ_Inv", function(this,x,
                                                  column,
                                                  asdf=TRUE) {
  verbose("OeQ_Inv: predict_spline **********",1)
  if(asdf==FALSE) return(this$.spline[[column]](x))
  return(data.frame(x=x,y=this$.spline[[column]](x),stringsAsFactors=FALSE))
  
})
setMethodS3("predict_splinelog", "OeQ_Inv", function(this,x,
                                                     column,
                                                     asdf=TRUE) {
  if(asdf==FALSE) return(this$.splinelog[[column]](log(x)))
  return(data.frame(x=x,y=this$.splinelog[[column]](log(x)),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm1", "OeQ_Inv", function(this,x,
                                               column,
                                               asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm1[[column]],newdata=data.frame(x=x)))
  return(data.frame(x=x,y=predict(this$.lm1[[column]],newdata=data.frame(x=x)),stringsAsFactors=FALSE))
  
})
setMethodS3("predict_lm2", "OeQ_Inv", function(this,x,
                                               column,
                                               asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm2[[column]],newdata=data.frame(x=x)))
  return(data.frame(x=x,y=predict(this$.lm2[[column]],newdata=data.frame(x=x)),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm3", "OeQ_Inv", function(this,x,
                                               column,
                                               asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm3[[column]],newdata=data.frame(x=x)))
  return(data.frame(x=x,y=predict(this$.lm3[[column]],newdata=data.frame(x=x)),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm1log", "OeQ_Inv", function(this,x,
                                                  column,
                                                  asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm1log[[column]],newdata=data.frame(x=log(x))))
  return(data.frame(x=x,y=predict(this$.lm1log[[column]],newdata=data.frame(x=log(x))),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm2log", "OeQ_Inv", function(this,x,
                                                  column,
                                                  asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm2log[[column]],newdata=data.frame(x=log(x))))
  return(data.frame(x=x,y=predict(this$.lm2log[[column]],newdata=data.frame(x=log(x))),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm3log", "OeQ_Inv", function(this,x,
                                                  column,
                                                  asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm3log[[column]],newdata=data.frame(x=log(x))))
  return(data.frame(x=x,y=predict(this$.lm3log[[column]],newdata=data.frame(x=log(x))),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm1_splined", "OeQ_Inv", function(this,x,
                                                       column,
                                                       asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm1_splined[[column]],newdata=data.frame(x=x)))
  return(data.frame(x=x,y=predict(this$.lm1_splined[[column]],newdata=data.frame(x=x)),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm2_splined", "OeQ_Inv", function(this,x,
                                                       column,
                                                       asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm2_splined[[column]],newdata=data.frame(x=x)))
  return(data.frame(x=x,y=predict(this$.lm2_splined[[column]],newdata=data.frame(x=x)),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm3_splined", "OeQ_Inv", function(this,x,
                                                       column,
                                                       asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm3_splined[[column]],newdata=data.frame(x=x)))
  return(data.frame(x=x,y=predict(this$.lm3_splined[[column]],newdata=data.frame(x=x)),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm1log_splined", "OeQ_Inv", function(this,x,
                                                          column,
                                                          asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm1log_splined[[column]],newdata=data.frame(x=log(x))))
  return(data.frame(x=x,y=predict(this$.lm1log_splined[[column]],newdata=data.frame(x=log(x))),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm2log_splined", "OeQ_Inv", function(this,x,
                                                          column,
                                                          asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm2log_splined[[column]],newdata=data.frame(x=log(x))))
  return(data.frame(x=x,y=predict(this$.lm2log_splined[[column]],newdata=data.frame(x=log(x))),stringsAsFactors=FALSE))
})
setMethodS3("predict_lm3log_splined", "OeQ_Inv", function(this,x,
                                                          column,
                                                          asdf=TRUE) {
  if(asdf==FALSE) return(predict(this$.lm3log_splined[[column]],newdata=data.frame(x=log(x))))
  return(data.frame(x=x,y=predict(this$.lm3log_splined[[column]],newdata=data.frame(x=log(x))),stringsAsFactors=FALSE))
})

setMethodS3("regression_combined", "OeQ_Inv", function(this,
                                                       columns=NULL,
                                                       ...) {
  lm_last2<-function(x,y,count=4,minn=0,maxn=1){
    ymean1=mean(y)
    ymean2=mean(y[(length(y)%/%2):(length(y))])
    ymean3=ymean2+(ymean2-ymean1)
    ymean4=ymean3+(ymean2-ymean1)
    ymean5=ymean4+(ymean2-ymean1)
    xout=c(max(x)*1.5,max(x)*2.5,max(x)*4)
    yout=c(ymean3,ymean4,ymean5)
    return(data.frame(x=xout,y=yout,stringsAsFactors=FALSE)) 
  }
  lm_last<-function(x,y,count=4,minn=0,maxn=1){
    #   plot(x,y)
    dist=(max(x)-min(x))/(length(x)-1)
    xout=seq(max(x),max(x)+dist*count,length.out=count)
    print(xout)
    x=x[(length(x)-count+1):(length(x))]
    y=y[(length(y)-count+1):(length(y))]
    print(data.frame(x=x,y=y))
    l.sm=smooth.spline(x,y,spar=0.6)
   # l.lm<-lm(y ~ poly(x,1,raw=TRUE))
    weight=1/exp(seq(0,2,length.out=count))
    weight=1
 #  plot(predict(l.lm,newdata=data.frame(x=xout)))
   plot(xout,predict(l.sm,xout))
#   if(l.lm$coeff[2]<0){
#      yout=minn+predict(l.lm,newdata=data.frame(x=xout))*weight
#      y2out=minn
#    }else{
#      yout=maxn*(1-weight)+predict(l.lm,newdata=data.frame(x=xout))*weight
#      y2out=maxn
#    }
    yout=predict(l.sm,xout)
    print(l.lm)
    #    plot(x,y)
    # print(this$.proportionalized["mean",this$.normcolumn,])
    #    exit
    print(data.frame(x=xout,y=yout))
    #    exit
    #    lines(x,y)
    return(list(lin=l.lm,data=data.frame(x=xout,y=yout,stringsAsFactors=FALSE)))  
    #  return(data.frame(x=xout,y=rep(y2out,length(xout)),stringsAsFactors=FALSE))  
  }
  if(is.null(columns)) columns=names(this$.lm1)
  xs=this$.proportionalized["mean",this$.keycolumn,]
  #if(this$.type=="log") {
  #    # avoid infinite values on log by setting 0 to 1e-99
  #    xs[xs==0]<-1e-99
  #    xs=log(xs) 
  #  }
  for (i in columns){
    x=this$.proportionalized["mean",this$.keycolumn,]
    y=this$.proportionalized["mean",i,]
    l.lastlm=lm_last2(x,y,5)#length(x)%/%2)
    print(l.lastlm)
 #   exit
    ymean=mean(y)
 #  x=c(x,l.lastlm$x)
 #  y=c(y,l.lastlm$y)
    plot(x,y)
    lines(x,y,col="red")
   lines(smooth.spline(x,y,df=6),col="green")
       ymean1=mean(y[(length(y)%/%2):(length(y))])
    lines(x,rep(ymean,length(x)))
    lines(x,rep(ymean1,length(x)),col="red")
    this$.lm1[[i]]<-lm(y ~ poly(x,3,raw=TRUE))
    this$.lm1[[i]]$verbose_print=expression(paste("f ( ",x," ) = Const ",+ ax + bx^2 + cx^3))
    this$.lm1[[i]]$verbose_scr="f ( x ) = Const + ax + bx^2 + cx^3"
    this$.lm1[[i]]$predictor=predict_lm1
   
    this$.lm2[[i]]<-lm(y ~ poly(x,4,raw=TRUE))
    this$.lm2[[i]]$verbose_print=expression(paste("f ( ",x," ) = Const ",+ ax + bx^2 + cx^3 + dx^4))
    this$.lm2[[i]]$verbose_scr="f ( x ) = Const + ax + bx^2 + cx^3 + dx^4"
    this$.lm2[[i]]$predictor=predict_lm2
  
    this$.lm3[[i]]<-lm(y ~ poly(x,5,raw=TRUE))
    this$.lm3[[i]]$verbose_print=expression(paste("f ( ",x," ) = Const ",+ ax + bx^2 + cx^3 + dx^4 + ex^5))
    this$.lm3[[i]]$verbose_scr="f ( x ) = Const + ax + bx^2 + cx^3 + dx^4 + ex^5"
    this$.lm3[[i]]$predictor=predict_lm3
    #    this$.nls2[[i]]<-nls(y ~ Const + A * x + B*I(x^2),start=list(Const=1,A=1,B=1),control=list(warnOnly=TRUE))
  # this$.spline[[i]]=splinefun(x,y,method = "natural")
   this$.spline[[i]]=smooth.spline(x,y,df=6)
    #x=runif(100,min(x),max(x)*3)
    #x=x[order(x)]
#    y=this$.spline[[i]](x)
    y=this$.spline[[i]](x)
    
    this$.lm1_splined[[i]]<-lm(y ~ poly(x,3,raw=TRUE))
    this$.lm1_splined[[i]]$verbose_print=expression(paste("f ( ",x," ) = Const ",+ ax + bx^2 + cx^3," (splined)"))
    this$.lm1_splined[[i]]$verbose_scr="f ( x ) = Const + ax + bx^2 + cx^3 (splined)"
    this$.lm1_splined[[i]]$predictor=predict_lm1_splined
    
    this$.lm2_splined[[i]]<-lm(y ~ poly(x,4,raw=TRUE))
    this$.lm2_splined[[i]]$verbose_scr="f ( x ) = Const + ax + bx^2 + cx^3 + dx^4 (splined)"
    this$.lm2_splined[[i]]$verbose_print=expression(paste("f ( ",x," ) = Const ",+ ax + bx^2 + cx^3 + dx^4," (splined)"))
    this$.lm2_splined[[i]]$predictor=predict_lm2_splined
    
    this$.lm3_splined[[i]]<-lm(y ~ poly(x,5,raw=TRUE))
    this$.lm3_splined[[i]]$verbose_scr="f ( x ) = Const + ax + bx^2 + cx^3 + dx^4 + ex^5 (splined)"
    this$.lm3_splined[[i]]$verbose_print=expression(paste("f ( ",x," ) = Const ",+ ax + bx^2 + cx^3 + dx^4 + ex^5," (splined)"))
    this$.lm3_splined[[i]]$predictor=predict_lm3_splined
  }
  for (i in columns){
    x=this$.proportionalized["mean",this$.keycolumn,]
    x=log(x)
    y=this$.proportionalized["mean",i,]
    l.lastlm=lm_last2(x,y,length(x)%/%2)
 #   x=c(x,l.lastlm$x)
  #  y=c(y,l.lastlm$y)
    plot(x,y)
    lines(x,y,col="red")
    lines(smooth.spline(x,y,df=6),col="green")
    ymean1=mean(y)
  #  lines(x,rep(ymean,length(x)))
    lines(x,rep(ymean1,length(x)))
    # x=log(xs)
    
    this$.lm1log[[i]]<-lm(y ~ poly(x,3,raw=TRUE))
    this$.lm1log[[i]]$verbose_print=expression(paste("f ( ",bar(x)," ) = Const + a",bar(x),"  + b ",bar(x)^2," + c",bar(x) ^3," ; ",bar(x) == ln(x)))
    this$.lm1log[[i]]$verbose_scr="f ( x' ) = Const + ax' + bx'^2 + cx'^3 ; x' = ln(x)"
    this$.lm1log[[i]]$predictor=predict_lm1log
    
    this$.lm2log[[i]]<-lm(y ~ poly(x,4,raw=TRUE))
    this$.lm2log[[i]]$verbose_print=expression(paste("f ( ",bar(x)," ) = Const + a",bar(x),"  + b ",bar(x)^2," + c",bar(x) ^3," + d",bar(x) ^4," ; ",bar(x) == ln(x)))
    this$.lm2log[[i]]$verbose_scr="f ( x' ) = Const + ax' + bx'^2 + cx'^3 + dx^4 ; x' = ln(x)"
    this$.lm2log[[i]]$predictor=predict_lm2log
    
    this$.lm3log[[i]]<-lm(y ~ poly(x,5,raw=TRUE))
    this$.lm3log[[i]]$verbose_print=expression(paste("f ( ",bar(x)," ) = Const + a",bar(x),"  + b ",bar(x)^2," + c",bar(x) ^3," + d",bar(x) ^4," + e",bar(x) ^5," ; ",bar(x) == ln(x)))
    this$.lm3log[[i]]$verbose_scr="f ( x' ) = Const + ax' + bx'^2 + cx'^3 + dx^4 + ex^5 ; x' = ln(x)"
    this$.lm3log[[i]]$predictor=predict_lm3log
    #    this$.nls2[[i]]<-nls(y ~ Const + A * x + B*I(x^2),start=list(Const=1,A=1,B=1),control=list(warnOnly=TRUE))
    this$.splinelog[[i]]=splinefun(x,y,method = "natural")
    #x=runif(100,min(x),max(x)*3)
    #x=x[order(x)]
    y=this$.splinelog[[i]](x)
    
    this$.lm1log_splined[[i]]<-lm(y ~ poly(x,3,raw=TRUE))
    this$.lm1log_splined[[i]]$verbose_print=expression(paste("f ( ",bar(x)," ) = Const + a",bar(x),"  + b ",bar(x)^2," + c",bar(x) ^3," ; ",bar(x) == ln(x), "(splined)"))
    this$.lm1log_splined[[i]]$verbose_scr="f ( x' ) = Const + ax' + bx'^2 + cx'^3 ; x' = ln(x) (splined)"
    this$.lm1log_splined[[i]]$predictor=predict_lm1log_splined
    
    this$.lm2log_splined[[i]]<-lm(y ~ poly(x,4,raw=TRUE))
    this$.lm2log_splined[[i]]$verbose_print=expression(paste("f ( ",bar(x)," ) = Const + a",bar(x),"  + b ",bar(x)^2," + c",bar(x) ^3," + d",bar(x) ^4," ; ",bar(x) == ln(x), "(splined)"))
    this$.lm2log[[i]]$verbose_scr="f ( x' ) = Const + ax' + bx'^2 + cx'^3 + dx'^4 ; x' = ln(x) (splined)"
    this$.lm2log_splined[[i]]$predictor=predict_lm2log_splined
    
    this$.lm3log_splined[[i]]<-lm(y ~ poly(x,5,raw=TRUE))
    this$.lm3log_splined[[i]]$verbose_print=expression(paste("f ( ",bar(x)," ) = Const + a",bar(x),"  + b ",bar(x)^2," + c",bar(x) ^3," + d",bar(x) ^4," + e",bar(x) ^5," ; ",bar(x) == ln(x), "(splined)"))
    this$.lm3log[[i]]$verbose_scr="f ( x' ) = Const + ax' + bx'^2 + cx'^3 + dx'^4 + ex'^5 ; x' = ln(x) (splined)"
    this$.lm3log_splined[[i]]$predictor=predict_lm3log_splined
  }
  this
})


setMethodS3("fastcompare", "OeQ_Inv", function(this,
                                               column=NULL,
                                               xrange=NULL,
                                               ...) {
  plot(this$predict_spline(xrange,column),type="n")
  #exit
  x=this$.proportionalized["mean",this$.keycolumn,]
  y=this$.proportionalized["mean",column,]
  points(x,y,pch=4)
  lines(this$predict_spline(xrange,column),)
  lines(this$predict_splinelog(xrange,column))
lines(this$predict_lm1(xrange,column),col="RED",lwd=2)
lines(this$predict_lm1_splined(xrange,column),col="RED",lty=2,lwd=2)
lines(this$predict_lm2(xrange,column),col="BLUE",lwd=2)
lines(this$predict_lm3(xrange,column),col="DARK GREEN",lwd=2)
lines(this$predict_lm2_splined(xrange,column),col="BLUE",lty=2,lwd=2)
lines(this$predict_lm3_splined(xrange,column),col="DARK GREEN",lty=2,lwd=2)
lines(this$predict_lm1log(xrange,column),col="RED",lty=3,lwd=2)
lines(this$predict_lm2log(xrange,column),col="BLUE",lty=3,lwd=2)
lines(this$predict_lm3log(xrange,column),col="DARK GREEN",lty=3,lwd=4)
lines(this$predict_lm1log_splined(xrange,column),col="RED",lty=4,lwd=2)
lines(this$predict_lm2log_splined(xrange,column),col="BLUE",lty=4,lwd=2)
lines(this$predict_lm3log_splined(xrange,column),col="DARK GREEN",lty=4,lwd=2)
#legend(0,0,legend=c("spline","splinelog"))
legend("topright", c("lm1", "lm1_splined","lm1log", "lm1log_splined",
                     "lm2", "lm2_splined", "lm2log", "lm2log_splined",
                     "lm3", "lm3_splined", "lm3log", "lm3log_splined",
                     paste("Least Square is:",this$leastsquaretest(column))), col = c("RED","RED","RED","RED","BLUE","BLUE","BLUE","BLUE","DARK GREEN","DARK GREEN","DARK GREEN","DARK GREEN"),
       text.col = "black", lty = c(1,2,3,4,1,2,3,4,1,2,3,4,0),lwd=2,
       merge = TRUE)})



setMethodS3("regression_smooth", "OeQ_Inv", function(this,
                                              columns=NULL,
                                              ...) {
  if(is.null(columns)) columns=names(this$.lm1)
  x=this$.proportionalized["mean",this$.keycolumn,]
  
  if(this$.type=="log") {
    # avoid infinite values on log by setting 0 to 1e-99
    x[x==0]<-1e-99
    x=log(x) 
  }
  #  plot(x)
  for (i in columns){
    y=this$.proportionalized["mean",i,]
    #  plot(exp(x),y)
    # if(!is.null(this$.normcolumn)) y=y/this$.data[,this$.normcolumn]  
    #nonlinear least square 1: constant and log(x)
    #   suppressWarnings({
    this$.nls1[[i]]<-nls(y ~ Const + A * x,start=list(Const=1,A=1),control=list(warnOnly=TRUE))
    #nonlinear least square 2: constant and x
    this$.nls2[[i]]<-nls(y ~ Const + A * x + B*I(x^2),start=list(Const=1,A=1,B=1),control=list(warnOnly=TRUE))
    #linear model 1: polnomial 2nd degree
    this$.lm1[[i]]<-lm(y ~ poly(x,2,raw=TRUE))
    #linear model 2: polnomial 3rd degree
    print(lm(y ~ poly(x,3,raw=TRUE)))
    this$.lm2[[i]]<-lm(y ~ poly(x,3,raw=TRUE))
    #  })
  }
  this
})





xy_smooth<-function(xin,y,range=NULL,zoom=1,degree=3,type="log"){
  # default range is full range
  if(is.null(range)) range=c(min(xin),max(xin))
  # default is log smooth
  if(type=="log") x=log(xin) else x=xin
  # regression
  y=y[is.finite(x)]
  x=x[is.finite(x)]
  x=x[is.finite(y)]
  y=y[is.finite(y)]
  x[x==0]=1e-6
  print(cbind(x,y))
  l.lm=lm(y ~ poly(x,degree,raw=TRUE))
  #x=log(seq(range[1],range[2],length.out = 100))
  # set x to a 100 element sequence on the range
  x=seq(range[1],range[2],length.out = 100)
  # default is log smooth
  if(type=="log") x=log(xin) else x=xin
  y=predict(l.lm,data.frame(x = x))*zoom
  # cbind(x=exp(x),y=y)
  cbind(x=xin,y=y)
}

setMethodS3("distribution_plot", "OeQ_Inv", function(this,
                                                     columns=NULL,                                                    
                                                     xrange=NULL,
                                                     palette=NULL,
                                                     log="",
                                                     sum_plot=TRUE,
                                                     pdffile=NULL,
                                                     ...) {
  
  op <- par()
  par(xpd=TRUE)
  if(!is.null(pdffile)) {
    pdf(paste(PDF_PATH,"/",pdffile,".pdf",sep=""),paper="a4", width = 0, height = 0)
    par(mfrow=c(2,1))
    par(mai=c(2.1, 0.5, 0.5, 0.5))#,omi = c(0.1,0.1, 0.1, 0.1)) #
    par(omi = c(0,0.5, 0.2, 0))
    par(cex=0.8)
    par(cex.main=1.1)
    
  }else{
    par(mar=c(15.1, 4.1, 4.1, 1.1))#,omi = c(0.1,0.1, 0.1, 0.1)) #
    par(oma = c(0,0.3, 0.2, 0))  
  }
  if(is.null(columns)) columns=colnames(this$.data)
  columns=columns[columns!=this$.keycolumn]
  if(length(columns)==1) columns=c(columns)
  l.perc_factor=1
  if(!is.null(this$.normcolumn)) {
    columns=columns[columns!=this$.normcolumn]
    l.perc_factor=100}
  if(is.null(xrange)) xrange=this$.xrange
  if(xrange[1]==-1) xrange[1]=min(this$.data[,this$.keycolumn])
  if(xrange[2]==-1) xrange[2]=max(this$.data[,this$.keycolumn])
  
  if(is.null(palette)) palette=this$.palette
  x=this$.data[,this$.keycolumn]
  #print(c(min(x),max(x)))
  x.mean=this$.proportionalized["mean",this$.keycolumn,]
  #print(c(min(x.mean),max(x.mean)))
  xpin=seq(xrange[1],xrange[2],length.out = 100)
  if(this$.type=="log") xp=log(xpin) else xp=xpin
  for (i in columns){
    y=this$.data[,i]*l.perc_factor
    if(!is.null(this$.normcolumn)) y=y/this$.data[,this$.normcolumn] 
    #  print(palette)
    y.0=this$.proportionalized["0%",i,]
    y.25=this$.proportionalized["25%",i,]
    y.50=this$.proportionalized["50%",i,]
    y.75=this$.proportionalized["75%",i,]
    y.100=this$.proportionalized["100%",i,]
    par(xpd=TRUE)
    if(this$.type=="log") l.titlesuffix=paste("f( ",unlist(VERBOSE[this$.keycolumn,]$label),")",sep="") else l.titlesuffix=paste("f(",unlist(VERBOSE[this$.keycolumn,]$label),")",sep="")
    if(!is.null(this$.normcolumn)) l.titleprefix="Percentage of " else l.titleprefix=""
    plot(x,y,lwd=0,
         main=paste(unlist(VERBOSE["CORR_PLOT_TITLE",]$title),"\n",l.titleprefix,unlist(VERBOSE[i,]$label)," = ",l.titlesuffix," ",this$.regressions$.bestfit,sep=""),
         xlab=paste(unlist(VERBOSE[this$.keycolumn,]$label),unlist(VERBOSE[this$.keycolumn,]$unit)),
         ylab=paste(unlist(VERBOSE[i,]$label),unlist(VERBOSE[i,]$unit)),xlim=xrange,log=log)#ylim=c(0,100),
    rect(par("usr")[1],par("usr")[3],par("usr")[2],par("usr")[4],col = palette$bg)
    smoothScatter(x,y, colramp = palette$ramp,log=log, add=T,nbin=128) 
    y.mean=this$.proportionalized["mean",i,]
    par(xpd=FALSE)
    points(x.mean,y.mean*l.perc_factor,pch=3,lty=palette$curv$mean.lty,lwd=palette$curv$mean.lwd,col=palette$curv$mean.col)
    lines(xy_smooth(x.mean,y.25,range=NULL,zoom=l.perc_factor,type=this$.type),type="l",lty=palette$curv$q25.lty,lwd=palette$curv$q25.lwd,col=palette$curv$q25.col)
    lines(xy_smooth(x.mean,y.75,range=NULL,zoom=l.perc_factor,type=this$.type),type="l",lty=palette$curv$q75.lty,lwd=palette$curv$q75.lwd,col=palette$curv$q75.col)
    lines(xy_smooth(x.mean,y.50,range=NULL,zoom=l.perc_factor,type=this$.type),type="l",lty=palette$curv$q50.lty,lwd=palette$curv$q50.lwd,col=palette$curv$q50.col)
    lines(xpin,this$.regressions[[i]]$bestpredict(xpin,asdf=FALSE)*l.perc_factor,type="l",lwd=palette$curv$regr4.lwd,lty=palette$curv$regr4.lty,col=palette$curv$regr4.col)
    if(!is.null(pdffile)) {
      l.lpos="bottomright" 
      l.linset=c(0.0,-0.6)
    }else{ 
      l.lpos="bottomright" 
      l.linset=c(0,-0.50)
    }
    
    legend(l.lpos,  bty="n",  
           c("Local averages",
             "25% Quantile",
             "50% Quantile",
             "75% Quantile",
             this$.regressions[[i]]$verb_prn(),
             paste("x = ",unlist(VERBOSE[this$.keycolumn,]$label),sep=""),
             paste("y = ",unlist(VERBOSE[i,]$label),sep=""),
             paste("Const = ",formatC(this$.regressions[[i]]$coeff()[1],digits=12),sep=""),
             paste("a = ",formatC(this$.regressions[[i]]$coeff()[2],digits=12)," ;  b = ",formatC(this$.regressions[[i]]$coeff()[3],digits=12),sep=""),
             paste("c = ",formatC(this$.regressions[[i]]$coeff()[4],digits=12)," ;  d = ",formatC(this$.regressions[[i]]$coeff()[5],digits=12),sep="")),
           
           #"\n x\'(x) = log(x); y(x) = C + ax +bx^2 +cx^3"), 
           xpd = NA,ncol=2,
           inset=l.linset,
           col=c(palette$curv$mean.col,
                 palette$curv$q25.col,
                 palette$curv$q50.col,
                 palette$curv$q75.col,
                 palette$curv$regr4.col,0,0,0,0,0,0),
           lty = c(palette$curv$mean.lty,
                   palette$curv$q25.lty,
                   palette$curv$q50.lty,
                   palette$curv$q75.lty,
                   palette$curv$regr4.lty,0,0,0,0,0,0),
           lwd = c(palette$curv$mean.lwd,
                   palette$curv$q25.lwd,
                   palette$curv$q50.lwd,
                   palette$curv$q75.lwd,
                   palette$curv$regr4.lwd,0,0,0,0,0,0), 
           pch = c(3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA),
           merge = TRUE, bg = "white",
           box.lwd=0,
           text.col=palette$curv$textcol) 
  }
  #  Sys.sleep(3)
  if(sum_plot) this$sum_plot(columns=columns,xrange=xrange,palette=palette,log=log)
  suppressWarnings(par(op))
  if(!is.null(pdffile)) {dev.off()
                         browseURL(paste(PDF_PATH,"/",pdffile,".pdf",sep=""))}
})

setMethodS3("sum_plot", "OeQ_Inv", function(this,
                                            columns=NULL,                                                    
                                            xrange=NULL,
                                            palette=NULL,
                                            log="",
                                            pdffile=NULL,
                                            ...) {
  op <- par()
  par(xpd=TRUE)
 # stop()
  if(!is.null(pdffile)) {
    pdf(paste(PDF_PATH,"/",pdffile,".pdf",sep=""),paper="a4", width = 0, height = 0)
    par(mfrow=c(2,1))
    par(mai=c(0.8, 0.8, 0.5, 0.5))#,omi = c(0.1,0.1, 0.1, 0.1)) #
    par(omi = c(0,0.5, 0.2, 0))
    par(cex=0.8)
    par(cex.main=1.1)
    
  }else{
    par(mar=c(5.1, 4.1, 4.1, 1.1))#,omi = c(0.1,0.1, 0.1, 0.1)) #
    #par(oma = c(0,0.3, 0.2, 0))  
    par(oma = c(1, 1, 1, 1))
  }
  #mar=c(15.1, 10.1, 10.1, 10.1)
  if(is.null(columns)) columns=colnames(this$.data)
  columns=columns[columns!=this$.keycolumn]
  if(length(columns)==1) columns=c(columns)
  l.perc_factor=1
  if(!is.null(this$.normcolumn)) {
    columns=columns[columns!=this$.normcolumn]
    l.perc_factor=100}
  if(is.null(xrange)) {
    xrange=this$.xrange
  }
 # readline()
  if(is.null(palette)) palette=this$.palette
  xpin=seq(xrange[1],xrange[2],length.out = 1000)
  if(this$.type=="log") xp=log(xpin) else xp=xpin
  l.y_out=this$.regressions[[columns[1]]]$bestpredict(xpin,asdf=FALSE)*l.perc_factor
  l.y_out=cbind(l.y_out)
  for (i in columns[-1]){
 #   print(this$.regressions[[i]])
    l.y_out=cbind(l.y_out,this$.regressions[[i]]$bestpredict(xpin,asdf=FALSE)*l.perc_factor)
  }
 #colnames(l.y_out)<-columns
#  print(l.y_out[1:10,])
  # plot.stacked(xp,l.y_out)
  # plot.stacked(xp,l.y_out,lwd=0,
  #        main=paste(unlist(VERBOSE["CORR_SUM_PLOT_TITLE",]$title),"\n",columns[1],sep=""),
  #        xlab=paste(unlist(VERBOSE[this$.keycolumn,]$label),unlist(VERBOSE[this$.keycolumn,]$unit)),
  #        ylab=paste(unlist(VERBOSE["CORR_SUM_PLOT_XLAB",]$title),"[%]"),xlim=xrange,ylim=c(0,100),log=log,col=palette)
  # lines(xp,predict(this$.nls1[[i]],newdata=data.frame(x=log(xp)))*l.perc_factor,type="l",lty=palette$curv$regr1.lty,lwd=palette$curv$regr1.lwd,col=palette$curv$regr1.col)
  # lines(xp,predict(this$.nls2[[i]],newdata=data.frame(x=log(xp)))*l.perc_factor,type="l",lty=palette$curv$regr2.lty,lwd=palette$curv$regr2.lwd,col=palette$curv$regr2.col)
  # lines(xp,predict(this$.lm1[[i]],newdata=data.frame(x=log(xp)))*l.perc_factor,type="l",lty=palette$curv$regr3.lty,lwd=palette$curv$regr3.lwd,col=palette$curv$regr3.col)
  plot(xpin,l.y_out[,1],lwd=0, main=paste(unlist(VERBOSE["CORR_SUM_PLOT_TITLE",]$title),sep=""),
       xlab=paste(unlist(VERBOSE[this$.keycolumn,]$label),unlist(VERBOSE[this$.keycolumn,]$unit)),
       ylab=paste(unlist(VERBOSE["CORR_SUM_PLOT_XLAB",]$title),"[%]"),xlim=xrange,ylim=c(0, (1+length(columns)/70)*max(rowSums(l.y_out))),log=log,col=rainbow(length(l.y_out[1,])))
  bottom=0*l.y_out[,1]
  for(i in 1:length(l.y_out[1,])){
    top=rowSums(as.matrix(l.y_out[,1:i]))
    polygon(c(xpin, rev(xpin)), c(top, rev(bottom)), border=NULL, col=rainbow(length(l.y_out[1,]))[i])
    bottom=top
  }
  l.legend.text=rev(unlist(VERBOSE[columns,]$info))
  if (is.null(l.legend.text)) l.legend.text=unlist(columns) 
  #abline(h=seq(0,200000, 10000), lty=3, col="grey")
  #print(unlist(VERBOSE[columns,]$label))
  legend("topleft",bty="n", legend=l.legend.text, ncol=3, inset = c(0.02,0), fill=rev(rainbow(length(l.y_out[1,]))),  bg="white", cex=0.8, col=rainbow(length(l.y_out[1,])))
  Sys.sleep(1)
  suppressWarnings(par(op))
  if(!is.null(pdffile)) {dev.off()
                         browseURL(paste(PDF_PATH,"/",pdffile,".pdf",sep=""))}
})



